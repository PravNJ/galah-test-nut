{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dr3_all = pd.read_csv(\"/data/praveen/results/dr3_normal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sobject_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>4450</th>\n",
       "      <th>4451</th>\n",
       "      <th>4452</th>\n",
       "      <th>4453</th>\n",
       "      <th>4454</th>\n",
       "      <th>4455</th>\n",
       "      <th>4456</th>\n",
       "      <th>4457</th>\n",
       "      <th>4458</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131116000501002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131116000501004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131116000501005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131116000501006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131116000501007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4461 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sobject_id    0    1    2    3    4    5    6    7    8  ...  4450  \\\n",
       "0  131116000501002  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "1  131116000501004  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "2  131116000501005  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "3  131116000501006  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "4  131116000501007  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "\n",
       "   4451  4452  4453  4454  4455  4456  4457  4458  label  \n",
       "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0     10  \n",
       "1   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0     10  \n",
       "2   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0     10  \n",
       "3   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0     10  \n",
       "4   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0     10  \n",
       "\n",
       "[5 rows x 4461 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dr3_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    581367\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dr3_all[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10908/1334331202.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_data_only = df_dr3_all.drop([\"sobject_id\",\"label\"],1)\n"
     ]
    }
   ],
   "source": [
    "df_data_only = df_dr3_all.drop([\"sobject_id\",\"label\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4449</th>\n",
       "      <th>4450</th>\n",
       "      <th>4451</th>\n",
       "      <th>4452</th>\n",
       "      <th>4453</th>\n",
       "      <th>4454</th>\n",
       "      <th>4455</th>\n",
       "      <th>4456</th>\n",
       "      <th>4457</th>\n",
       "      <th>4458</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  4449  4450  4451  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "1  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "2  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "3  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "4  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "\n",
       "   4452  4453  4454  4455  4456  4457  4458  \n",
       "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "1   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "2   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "3   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "4   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "\n",
       "[5 rows x 4459 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_only_inverted = 1 - df_data_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras import backend as K\n",
    "#from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = None  # PReLU if set to None\n",
    "dropout_rate = 0  # from 0 to 1\n",
    "n_l_e = 5\n",
    "n_epoch = 350\n",
    "decoded_layer_name = 'encoded'\n",
    "n_wvl = 4459\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# compute number of nodes in every connected layer\n",
    "n_l_1 = int(n_wvl * 0.75)\n",
    "n_l_2 = int(n_wvl * 0.50)\n",
    "n_l_3 = 0  # int(n_wvl * 0.40)\n",
    "n_l_4 = 0  # int(n_wvl * 0.20)\n",
    "n_l_5 = int(n_wvl * 0.25)\n",
    "n_l_6 = int(n_wvl * 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:09:09.640660: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " E_1 (Dense)                 (None, 3344)              14914240  \n",
      "                                                                 \n",
      " PR_1 (PReLU)                (None, 3344)              3344      \n",
      "                                                                 \n",
      " E_2 (Dense)                 (None, 2229)              7456005   \n",
      "                                                                 \n",
      " PR_2 (PReLU)                (None, 2229)              2229      \n",
      "                                                                 \n",
      " E_5 (Dense)                 (None, 1114)              2484220   \n",
      "                                                                 \n",
      " PR_5 (PReLU)                (None, 1114)              1114      \n",
      "                                                                 \n",
      " E_6 (Dense)                 (None, 445)               496175    \n",
      "                                                                 \n",
      " PR_6 (PReLU)                (None, 445)               445       \n",
      "                                                                 \n",
      " encoded (Dense)             (None, 5)                 2230      \n",
      "                                                                 \n",
      " PR_7 (PReLU)                (None, 5)                 5         \n",
      "                                                                 \n",
      " D_1 (Dense)                 (None, 445)               2670      \n",
      "                                                                 \n",
      " PR_8 (PReLU)                (None, 445)               445       \n",
      "                                                                 \n",
      " D_2 (Dense)                 (None, 1114)              496844    \n",
      "                                                                 \n",
      " PR_9 (PReLU)                (None, 1114)              1114      \n",
      "                                                                 \n",
      " D_5 (Dense)                 (None, 2229)              2485335   \n",
      "                                                                 \n",
      " PR_12 (PReLU)               (None, 2229)              2229      \n",
      "                                                                 \n",
      " D_6 (Dense)                 (None, 3344)              7457120   \n",
      "                                                                 \n",
      " PR_13 (PReLU)               (None, 3344)              3344      \n",
      "                                                                 \n",
      " recreated (Dense)           (None, 4459)              14915355  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,724,463\n",
      "Trainable params: 50,724,463\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Sequential()\n",
    "\n",
    "if n_l_1 > 0:\n",
    "    autoencoder.add(Dense(n_l_1, input_shape=(n_wvl,), activation=activation, name='E_1'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_1'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_1'))\n",
    "\n",
    "if n_l_2 > 0:\n",
    "    autoencoder.add(Dense(n_l_2, activation=activation, name='E_2'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_2'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_2'))\n",
    "\n",
    "if n_l_3 > 0:\n",
    "    autoencoder.add(Dense(n_l_3, activation=activation, name='E_3'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_3'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_3'))\n",
    "\n",
    "if n_l_4 > 0:\n",
    "    autoencoder.add(Dense(n_l_4, activation=activation, name='E_4'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_4'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_4'))\n",
    "\n",
    "if n_l_5 > 0:\n",
    "    autoencoder.add(Dense(n_l_5, activation=activation, name='E_5'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_5'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_5'))\n",
    "\n",
    "if n_l_6 > 0:\n",
    "    autoencoder.add(Dense(n_l_6, activation=activation, name='E_6'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_6'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_6'))\n",
    "\n",
    "autoencoder.add(Dense(n_l_e, activation=activation, name=decoded_layer_name))\n",
    "if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_7'))\n",
    "\n",
    "if n_l_6 > 0:\n",
    "    autoencoder.add(Dense(n_l_6, activation=activation, name='D_1'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_8'))\n",
    "    if activation is None:\n",
    "            autoencoder.add(PReLU(name='PR_8'))\n",
    "\n",
    "if n_l_5 > 0:\n",
    "    autoencoder.add(Dense(n_l_5, activation=activation, name='D_2'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_9'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_9'))\n",
    "\n",
    "if n_l_4 > 0:\n",
    "    autoencoder.add(Dense(n_l_4, activation=activation, name='D_3'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_10'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_10'))\n",
    "\n",
    "if n_l_3 > 0:\n",
    "    autoencoder.add(Dense(n_l_3, activation=activation, name='D_4'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_11'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_11'))\n",
    "\n",
    "if n_l_2 > 0:\n",
    "    autoencoder.add(Dense(n_l_2, activation=activation, name='D_5'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_12'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_12'))\n",
    "\n",
    "if n_l_1 > 0:\n",
    "    autoencoder.add(Dense(n_l_1, activation=activation, name='D_6'))\n",
    "    if dropout_rate > 0:\n",
    "        autoencoder.add(Dropout(dropout_rate, name='DO_13'))\n",
    "    if activation is None:\n",
    "        autoencoder.add(PReLU(name='PR_13'))\n",
    "\n",
    "autoencoder.add(Dense(n_wvl, activation='linear', name='recreated'))\n",
    "autoencoder.summary()\n",
    "\n",
    "# Visualize network architecture and save the visualization as a file\n",
    "#plot_model(autoencoder, show_layer_names=True, show_shapes=True, to_file='ann_network_structure_a.pdf')\n",
    "#plot_model(autoencoder, show_layer_names=True, show_shapes=True, to_file='ann_network_structure_a.png', dpi=300)\n",
    "\n",
    "# model file handling\n",
    "out_model_file = 'model_weights.h5'\n",
    "\n",
    "if os.path.isfile(out_model_file):\n",
    "    autoencoder.load_weights(out_model_file, by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=optimizer, loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('/data/praveen/autoencoder-run-2/'+'ann_model_run_{epoch:03d}-{loss:.4f}-{val_loss:.4f}.h5',\n",
    "                                     monitor='val_loss', verbose=0, save_best_only=False,\n",
    "                                     save_weights_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:58:41.179820: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18664660560 exceeds 10% of free system memory.\n",
      "2022-04-18 11:02:18.765669: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18664660560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 11:02:52.069524: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1426880000 exceeds 10% of free system memory.\n",
      "2022-04-18 11:02:52.069608: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1426880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 341s - loss: 0.0352 - val_loss: 0.0289 - 341s/epoch - 24s/step\n",
      "Epoch 2/350\n",
      "14/14 - 275s - loss: 0.0276 - val_loss: 0.0265 - 275s/epoch - 20s/step\n",
      "Epoch 3/350\n",
      "14/14 - 290s - loss: 0.0259 - val_loss: 0.0255 - 290s/epoch - 21s/step\n",
      "Epoch 4/350\n",
      "14/14 - 289s - loss: 0.0247 - val_loss: 0.0231 - 289s/epoch - 21s/step\n",
      "Epoch 5/350\n",
      "14/14 - 286s - loss: 0.0213 - val_loss: 0.0196 - 286s/epoch - 20s/step\n",
      "Epoch 6/350\n",
      "14/14 - 289s - loss: 0.0191 - val_loss: 0.0185 - 289s/epoch - 21s/step\n",
      "Epoch 7/350\n",
      "14/14 - 291s - loss: 0.0183 - val_loss: 0.0182 - 291s/epoch - 21s/step\n",
      "Epoch 8/350\n",
      "14/14 - 286s - loss: 0.0180 - val_loss: 0.0180 - 286s/epoch - 20s/step\n",
      "Epoch 9/350\n",
      "14/14 - 289s - loss: 0.0179 - val_loss: 0.0179 - 289s/epoch - 21s/step\n",
      "Epoch 10/350\n",
      "14/14 - 292s - loss: 0.0178 - val_loss: 0.0179 - 292s/epoch - 21s/step\n",
      "Epoch 11/350\n",
      "14/14 - 283s - loss: 0.0178 - val_loss: 0.0178 - 283s/epoch - 20s/step\n",
      "Epoch 12/350\n",
      "14/14 - 288s - loss: 0.0177 - val_loss: 0.0178 - 288s/epoch - 21s/step\n",
      "Epoch 13/350\n",
      "14/14 - 290s - loss: 0.0177 - val_loss: 0.0178 - 290s/epoch - 21s/step\n",
      "Epoch 14/350\n",
      "14/14 - 282s - loss: 0.0177 - val_loss: 0.0178 - 282s/epoch - 20s/step\n",
      "Epoch 15/350\n",
      "14/14 - 285s - loss: 0.0177 - val_loss: 0.0177 - 285s/epoch - 20s/step\n",
      "Epoch 16/350\n",
      "14/14 - 293s - loss: 0.0176 - val_loss: 0.0177 - 293s/epoch - 21s/step\n",
      "Epoch 17/350\n",
      "14/14 - 280s - loss: 0.0176 - val_loss: 0.0178 - 280s/epoch - 20s/step\n",
      "Epoch 18/350\n",
      "14/14 - 284s - loss: 0.0176 - val_loss: 0.0178 - 284s/epoch - 20s/step\n",
      "Epoch 19/350\n",
      "14/14 - 274s - loss: 0.0176 - val_loss: 0.0178 - 274s/epoch - 20s/step\n",
      "Epoch 20/350\n",
      "14/14 - 277s - loss: 0.0176 - val_loss: 0.0177 - 277s/epoch - 20s/step\n",
      "Epoch 21/350\n",
      "14/14 - 275s - loss: 0.0176 - val_loss: 0.0178 - 275s/epoch - 20s/step\n",
      "Epoch 22/350\n",
      "14/14 - 262s - loss: 0.0176 - val_loss: 0.0177 - 262s/epoch - 19s/step\n",
      "Epoch 23/350\n",
      "14/14 - 279s - loss: 0.0176 - val_loss: 0.0177 - 279s/epoch - 20s/step\n",
      "Epoch 24/350\n",
      "14/14 - 284s - loss: 0.0176 - val_loss: 0.0177 - 284s/epoch - 20s/step\n",
      "Epoch 25/350\n",
      "14/14 - 284s - loss: 0.0176 - val_loss: 0.0177 - 284s/epoch - 20s/step\n",
      "Epoch 26/350\n",
      "14/14 - 289s - loss: 0.0176 - val_loss: 0.0178 - 289s/epoch - 21s/step\n",
      "Epoch 27/350\n",
      "14/14 - 286s - loss: 0.0176 - val_loss: 0.0177 - 286s/epoch - 20s/step\n",
      "Epoch 28/350\n",
      "14/14 - 283s - loss: 0.0176 - val_loss: 0.0177 - 283s/epoch - 20s/step\n",
      "Epoch 29/350\n",
      "14/14 - 290s - loss: 0.0176 - val_loss: 0.0176 - 290s/epoch - 21s/step\n",
      "Epoch 30/350\n",
      "14/14 - 288s - loss: 0.0176 - val_loss: 0.0176 - 288s/epoch - 21s/step\n",
      "Epoch 31/350\n",
      "14/14 - 289s - loss: 0.0175 - val_loss: 0.0176 - 289s/epoch - 21s/step\n",
      "Epoch 32/350\n",
      "14/14 - 280s - loss: 0.0176 - val_loss: 0.0177 - 280s/epoch - 20s/step\n",
      "Epoch 33/350\n",
      "14/14 - 286s - loss: 0.0176 - val_loss: 0.0177 - 286s/epoch - 20s/step\n",
      "Epoch 34/350\n",
      "14/14 - 283s - loss: 0.0176 - val_loss: 0.0177 - 283s/epoch - 20s/step\n",
      "Epoch 35/350\n",
      "14/14 - 290s - loss: 0.0175 - val_loss: 0.0177 - 290s/epoch - 21s/step\n",
      "Epoch 36/350\n",
      "14/14 - 282s - loss: 0.0176 - val_loss: 0.0177 - 282s/epoch - 20s/step\n",
      "Epoch 37/350\n",
      "14/14 - 288s - loss: 0.0176 - val_loss: 0.0177 - 288s/epoch - 21s/step\n",
      "Epoch 38/350\n",
      "14/14 - 287s - loss: 0.0177 - val_loss: 0.0190 - 287s/epoch - 21s/step\n",
      "Epoch 39/350\n",
      "14/14 - 287s - loss: 0.0185 - val_loss: 0.0182 - 287s/epoch - 20s/step\n",
      "Epoch 40/350\n",
      "14/14 - 281s - loss: 0.0177 - val_loss: 0.0177 - 281s/epoch - 20s/step\n",
      "Epoch 41/350\n",
      "14/14 - 286s - loss: 0.0175 - val_loss: 0.0176 - 286s/epoch - 20s/step\n",
      "Epoch 42/350\n",
      "14/14 - 290s - loss: 0.0174 - val_loss: 0.0173 - 290s/epoch - 21s/step\n",
      "Epoch 43/350\n",
      "14/14 - 285s - loss: 0.0177 - val_loss: 0.0175 - 285s/epoch - 20s/step\n",
      "Epoch 44/350\n",
      "14/14 - 282s - loss: 0.0172 - val_loss: 0.0173 - 282s/epoch - 20s/step\n",
      "Epoch 45/350\n",
      "14/14 - 283s - loss: 0.0167 - val_loss: 0.0165 - 283s/epoch - 20s/step\n",
      "Epoch 46/350\n",
      "14/14 - 286s - loss: 0.0165 - val_loss: 0.0170 - 286s/epoch - 20s/step\n",
      "Epoch 47/350\n",
      "14/14 - 280s - loss: 0.0166 - val_loss: 0.0163 - 280s/epoch - 20s/step\n",
      "Epoch 48/350\n",
      "14/14 - 290s - loss: 0.0161 - val_loss: 0.0161 - 290s/epoch - 21s/step\n",
      "Epoch 49/350\n",
      "14/14 - 287s - loss: 0.0160 - val_loss: 0.0159 - 287s/epoch - 20s/step\n",
      "Epoch 50/350\n",
      "14/14 - 286s - loss: 0.0159 - val_loss: 0.0160 - 286s/epoch - 20s/step\n",
      "Epoch 51/350\n",
      "14/14 - 287s - loss: 0.0158 - val_loss: 0.0158 - 287s/epoch - 21s/step\n",
      "Epoch 52/350\n",
      "14/14 - 286s - loss: 0.0157 - val_loss: 0.0159 - 286s/epoch - 20s/step\n",
      "Epoch 53/350\n",
      "14/14 - 280s - loss: 0.0158 - val_loss: 0.0157 - 280s/epoch - 20s/step\n",
      "Epoch 54/350\n",
      "14/14 - 289s - loss: 0.0157 - val_loss: 0.0158 - 289s/epoch - 21s/step\n",
      "Epoch 55/350\n",
      "14/14 - 284s - loss: 0.0156 - val_loss: 0.0157 - 284s/epoch - 20s/step\n",
      "Epoch 56/350\n",
      "14/14 - 279s - loss: 0.0158 - val_loss: 0.0156 - 279s/epoch - 20s/step\n",
      "Epoch 57/350\n",
      "14/14 - 289s - loss: 0.0155 - val_loss: 0.0156 - 289s/epoch - 21s/step\n",
      "Epoch 58/350\n",
      "14/14 - 284s - loss: 0.0155 - val_loss: 0.0162 - 284s/epoch - 20s/step\n",
      "Epoch 59/350\n",
      "14/14 - 282s - loss: 0.0157 - val_loss: 0.0156 - 282s/epoch - 20s/step\n",
      "Epoch 60/350\n",
      "14/14 - 286s - loss: 0.0155 - val_loss: 0.0155 - 286s/epoch - 20s/step\n",
      "Epoch 61/350\n",
      "14/14 - 285s - loss: 0.0155 - val_loss: 0.0155 - 285s/epoch - 20s/step\n",
      "Epoch 62/350\n",
      "14/14 - 286s - loss: 0.0154 - val_loss: 0.0155 - 286s/epoch - 20s/step\n",
      "Epoch 63/350\n",
      "14/14 - 287s - loss: 0.0155 - val_loss: 0.0158 - 287s/epoch - 20s/step\n",
      "Epoch 64/350\n",
      "14/14 - 287s - loss: 0.0157 - val_loss: 0.0156 - 287s/epoch - 21s/step\n",
      "Epoch 65/350\n",
      "14/14 - 290s - loss: 0.0157 - val_loss: 0.0155 - 290s/epoch - 21s/step\n",
      "Epoch 66/350\n",
      "14/14 - 289s - loss: 0.0154 - val_loss: 0.0157 - 289s/epoch - 21s/step\n",
      "Epoch 67/350\n",
      "14/14 - 287s - loss: 0.0155 - val_loss: 0.0154 - 287s/epoch - 21s/step\n",
      "Epoch 68/350\n",
      "14/14 - 284s - loss: 0.0154 - val_loss: 0.0154 - 284s/epoch - 20s/step\n",
      "Epoch 69/350\n",
      "14/14 - 282s - loss: 0.0154 - val_loss: 0.0157 - 282s/epoch - 20s/step\n",
      "Epoch 70/350\n",
      "14/14 - 286s - loss: 0.0154 - val_loss: 0.0154 - 286s/epoch - 20s/step\n",
      "Epoch 71/350\n",
      "14/14 - 285s - loss: 0.0153 - val_loss: 0.0154 - 285s/epoch - 20s/step\n",
      "Epoch 72/350\n",
      "14/14 - 283s - loss: 0.0152 - val_loss: 0.0152 - 283s/epoch - 20s/step\n",
      "Epoch 73/350\n",
      "14/14 - 290s - loss: 0.0152 - val_loss: 0.0152 - 290s/epoch - 21s/step\n",
      "Epoch 74/350\n",
      "14/14 - 278s - loss: 0.0152 - val_loss: 0.0152 - 278s/epoch - 20s/step\n",
      "Epoch 75/350\n",
      "14/14 - 279s - loss: 0.0152 - val_loss: 0.0153 - 279s/epoch - 20s/step\n",
      "Epoch 76/350\n",
      "14/14 - 282s - loss: 0.0151 - val_loss: 0.0152 - 282s/epoch - 20s/step\n",
      "Epoch 77/350\n",
      "14/14 - 273s - loss: 0.0151 - val_loss: 0.0153 - 273s/epoch - 20s/step\n",
      "Epoch 78/350\n",
      "14/14 - 277s - loss: 0.0151 - val_loss: 0.0152 - 277s/epoch - 20s/step\n",
      "Epoch 79/350\n",
      "14/14 - 273s - loss: 0.0151 - val_loss: 0.0151 - 273s/epoch - 20s/step\n",
      "Epoch 80/350\n",
      "14/14 - 276s - loss: 0.0150 - val_loss: 0.0151 - 276s/epoch - 20s/step\n",
      "Epoch 81/350\n",
      "14/14 - 280s - loss: 0.0150 - val_loss: 0.0150 - 280s/epoch - 20s/step\n",
      "Epoch 82/350\n",
      "14/14 - 273s - loss: 0.0150 - val_loss: 0.0150 - 273s/epoch - 19s/step\n",
      "Epoch 83/350\n",
      "14/14 - 282s - loss: 0.0150 - val_loss: 0.0150 - 282s/epoch - 20s/step\n",
      "Epoch 84/350\n",
      "14/14 - 282s - loss: 0.0150 - val_loss: 0.0150 - 282s/epoch - 20s/step\n",
      "Epoch 85/350\n",
      "14/14 - 280s - loss: 0.0150 - val_loss: 0.0150 - 280s/epoch - 20s/step\n",
      "Epoch 86/350\n",
      "14/14 - 282s - loss: 0.0150 - val_loss: 0.0150 - 282s/epoch - 20s/step\n",
      "Epoch 87/350\n",
      "14/14 - 283s - loss: 0.0150 - val_loss: 0.0150 - 283s/epoch - 20s/step\n",
      "Epoch 88/350\n",
      "14/14 - 282s - loss: 0.0150 - val_loss: 0.0150 - 282s/epoch - 20s/step\n",
      "Epoch 89/350\n",
      "14/14 - 285s - loss: 0.0150 - val_loss: 0.0150 - 285s/epoch - 20s/step\n",
      "Epoch 90/350\n",
      "14/14 - 283s - loss: 0.0149 - val_loss: 0.0150 - 283s/epoch - 20s/step\n",
      "Epoch 91/350\n",
      "14/14 - 284s - loss: 0.0149 - val_loss: 0.0150 - 284s/epoch - 20s/step\n",
      "Epoch 92/350\n",
      "14/14 - 288s - loss: 0.0149 - val_loss: 0.0149 - 288s/epoch - 21s/step\n",
      "Epoch 93/350\n",
      "14/14 - 294s - loss: 0.0149 - val_loss: 0.0150 - 294s/epoch - 21s/step\n",
      "Epoch 94/350\n",
      "14/14 - 285s - loss: 0.0149 - val_loss: 0.0150 - 285s/epoch - 20s/step\n",
      "Epoch 95/350\n",
      "14/14 - 282s - loss: 0.0149 - val_loss: 0.0150 - 282s/epoch - 20s/step\n",
      "Epoch 96/350\n",
      "14/14 - 284s - loss: 0.0149 - val_loss: 0.0150 - 284s/epoch - 20s/step\n",
      "Epoch 97/350\n",
      "14/14 - 288s - loss: 0.0149 - val_loss: 0.0149 - 288s/epoch - 21s/step\n",
      "Epoch 98/350\n",
      "14/14 - 286s - loss: 0.0149 - val_loss: 0.0149 - 286s/epoch - 20s/step\n",
      "Epoch 99/350\n",
      "14/14 - 287s - loss: 0.0149 - val_loss: 0.0149 - 287s/epoch - 21s/step\n",
      "Epoch 100/350\n",
      "14/14 - 294s - loss: 0.0149 - val_loss: 0.0149 - 294s/epoch - 21s/step\n",
      "Epoch 101/350\n",
      "14/14 - 285s - loss: 0.0149 - val_loss: 0.0150 - 285s/epoch - 20s/step\n",
      "Epoch 102/350\n",
      "14/14 - 289s - loss: 0.0149 - val_loss: 0.0149 - 289s/epoch - 21s/step\n",
      "Epoch 103/350\n",
      "14/14 - 282s - loss: 0.0149 - val_loss: 0.0149 - 282s/epoch - 20s/step\n",
      "Epoch 104/350\n",
      "14/14 - 284s - loss: 0.0150 - val_loss: 0.0149 - 284s/epoch - 20s/step\n",
      "Epoch 105/350\n",
      "14/14 - 285s - loss: 0.0149 - val_loss: 0.0150 - 285s/epoch - 20s/step\n",
      "Epoch 106/350\n",
      "14/14 - 291s - loss: 0.0149 - val_loss: 0.0149 - 291s/epoch - 21s/step\n",
      "Epoch 107/350\n",
      "14/14 - 295s - loss: 0.0149 - val_loss: 0.0149 - 295s/epoch - 21s/step\n",
      "Epoch 108/350\n",
      "14/14 - 284s - loss: 0.0149 - val_loss: 0.0150 - 284s/epoch - 20s/step\n",
      "Epoch 109/350\n",
      "14/14 - 286s - loss: 0.0149 - val_loss: 0.0149 - 286s/epoch - 20s/step\n",
      "Epoch 110/350\n",
      "14/14 - 285s - loss: 0.0149 - val_loss: 0.0149 - 285s/epoch - 20s/step\n",
      "Epoch 111/350\n",
      "14/14 - 291s - loss: 0.0149 - val_loss: 0.0149 - 291s/epoch - 21s/step\n",
      "Epoch 112/350\n",
      "14/14 - 290s - loss: 0.0149 - val_loss: 0.0149 - 290s/epoch - 21s/step\n",
      "Epoch 113/350\n",
      "14/14 - 288s - loss: 0.0149 - val_loss: 0.0149 - 288s/epoch - 21s/step\n",
      "Epoch 114/350\n",
      "14/14 - 282s - loss: 0.0149 - val_loss: 0.0149 - 282s/epoch - 20s/step\n",
      "Epoch 115/350\n",
      "14/14 - 284s - loss: 0.0148 - val_loss: 0.0151 - 284s/epoch - 20s/step\n",
      "Epoch 116/350\n",
      "14/14 - 284s - loss: 0.0150 - val_loss: 0.0150 - 284s/epoch - 20s/step\n",
      "Epoch 117/350\n",
      "14/14 - 285s - loss: 0.0149 - val_loss: 0.0149 - 285s/epoch - 20s/step\n",
      "Epoch 118/350\n",
      "14/14 - 289s - loss: 0.0149 - val_loss: 0.0149 - 289s/epoch - 21s/step\n",
      "Epoch 119/350\n",
      "14/14 - 289s - loss: 0.0148 - val_loss: 0.0149 - 289s/epoch - 21s/step\n",
      "Epoch 120/350\n",
      "14/14 - 290s - loss: 0.0149 - val_loss: 0.0149 - 290s/epoch - 21s/step\n",
      "Epoch 121/350\n",
      "14/14 - 286s - loss: 0.0148 - val_loss: 0.0148 - 286s/epoch - 20s/step\n",
      "Epoch 122/350\n",
      "14/14 - 293s - loss: 0.0148 - val_loss: 0.0149 - 293s/epoch - 21s/step\n",
      "Epoch 123/350\n",
      "14/14 - 286s - loss: 0.0148 - val_loss: 0.0148 - 286s/epoch - 20s/step\n",
      "Epoch 124/350\n",
      "14/14 - 287s - loss: 0.0148 - val_loss: 0.0150 - 287s/epoch - 21s/step\n",
      "Epoch 125/350\n",
      "14/14 - 286s - loss: 0.0148 - val_loss: 0.0148 - 286s/epoch - 20s/step\n",
      "Epoch 126/350\n",
      "14/14 - 285s - loss: 0.0148 - val_loss: 0.0150 - 285s/epoch - 20s/step\n",
      "Epoch 127/350\n",
      "14/14 - 289s - loss: 0.0148 - val_loss: 0.0149 - 289s/epoch - 21s/step\n",
      "Epoch 128/350\n",
      "14/14 - 287s - loss: 0.0148 - val_loss: 0.0148 - 287s/epoch - 21s/step\n",
      "Epoch 129/350\n",
      "14/14 - 289s - loss: 0.0148 - val_loss: 0.0149 - 289s/epoch - 21s/step\n",
      "Epoch 130/350\n",
      "14/14 - 287s - loss: 0.0148 - val_loss: 0.0148 - 287s/epoch - 20s/step\n",
      "Epoch 131/350\n",
      "14/14 - 286s - loss: 0.0147 - val_loss: 0.0148 - 286s/epoch - 20s/step\n",
      "Epoch 132/350\n",
      "14/14 - 289s - loss: 0.0148 - val_loss: 0.0148 - 289s/epoch - 21s/step\n",
      "Epoch 133/350\n",
      "14/14 - 289s - loss: 0.0148 - val_loss: 0.0149 - 289s/epoch - 21s/step\n",
      "Epoch 134/350\n",
      "14/14 - 291s - loss: 0.0148 - val_loss: 0.0148 - 291s/epoch - 21s/step\n",
      "Epoch 135/350\n",
      "14/14 - 286s - loss: 0.0148 - val_loss: 0.0148 - 286s/epoch - 20s/step\n",
      "Epoch 136/350\n",
      "14/14 - 288s - loss: 0.0148 - val_loss: 0.0149 - 288s/epoch - 21s/step\n",
      "Epoch 137/350\n",
      "14/14 - 294s - loss: 0.0147 - val_loss: 0.0148 - 294s/epoch - 21s/step\n",
      "Epoch 138/350\n",
      "14/14 - 291s - loss: 0.0148 - val_loss: 0.0149 - 291s/epoch - 21s/step\n",
      "Epoch 139/350\n",
      "14/14 - 286s - loss: 0.0147 - val_loss: 0.0148 - 286s/epoch - 20s/step\n",
      "Epoch 140/350\n",
      "14/14 - 290s - loss: 0.0147 - val_loss: 0.0148 - 290s/epoch - 21s/step\n",
      "Epoch 141/350\n",
      "14/14 - 287s - loss: 0.0148 - val_loss: 0.0148 - 287s/epoch - 20s/step\n",
      "Epoch 142/350\n",
      "14/14 - 288s - loss: 0.0147 - val_loss: 0.0148 - 288s/epoch - 21s/step\n",
      "Epoch 143/350\n",
      "14/14 - 284s - loss: 0.0147 - val_loss: 0.0148 - 284s/epoch - 20s/step\n",
      "Epoch 144/350\n",
      "14/14 - 286s - loss: 0.0148 - val_loss: 0.0147 - 286s/epoch - 20s/step\n",
      "Epoch 145/350\n",
      "14/14 - 284s - loss: 0.0147 - val_loss: 0.0148 - 284s/epoch - 20s/step\n",
      "Epoch 146/350\n",
      "14/14 - 282s - loss: 0.0148 - val_loss: 0.0148 - 282s/epoch - 20s/step\n",
      "Epoch 147/350\n",
      "14/14 - 286s - loss: 0.0147 - val_loss: 0.0147 - 286s/epoch - 20s/step\n",
      "Epoch 148/350\n",
      "14/14 - 287s - loss: 0.0148 - val_loss: 0.0148 - 287s/epoch - 21s/step\n",
      "Epoch 149/350\n",
      "14/14 - 290s - loss: 0.0147 - val_loss: 0.0147 - 290s/epoch - 21s/step\n",
      "Epoch 150/350\n",
      "14/14 - 287s - loss: 0.0146 - val_loss: 0.0150 - 287s/epoch - 20s/step\n",
      "Epoch 151/350\n",
      "14/14 - 280s - loss: 0.0148 - val_loss: 0.0149 - 280s/epoch - 20s/step\n",
      "Epoch 152/350\n",
      "14/14 - 283s - loss: 0.0147 - val_loss: 0.0147 - 283s/epoch - 20s/step\n",
      "Epoch 153/350\n",
      "14/14 - 297s - loss: 0.0146 - val_loss: 0.0146 - 297s/epoch - 21s/step\n",
      "Epoch 154/350\n",
      "14/14 - 285s - loss: 0.0146 - val_loss: 0.0147 - 285s/epoch - 20s/step\n",
      "Epoch 155/350\n",
      "14/14 - 290s - loss: 0.0146 - val_loss: 0.0146 - 290s/epoch - 21s/step\n",
      "Epoch 156/350\n",
      "14/14 - 286s - loss: 0.0146 - val_loss: 0.0146 - 286s/epoch - 20s/step\n",
      "Epoch 157/350\n",
      "14/14 - 281s - loss: 0.0145 - val_loss: 0.0146 - 281s/epoch - 20s/step\n",
      "Epoch 158/350\n",
      "14/14 - 290s - loss: 0.0146 - val_loss: 0.0147 - 290s/epoch - 21s/step\n",
      "Epoch 159/350\n",
      "14/14 - 292s - loss: 0.0145 - val_loss: 0.0145 - 292s/epoch - 21s/step\n",
      "Epoch 160/350\n",
      "14/14 - 285s - loss: 0.0145 - val_loss: 0.0147 - 285s/epoch - 20s/step\n",
      "Epoch 161/350\n",
      "14/14 - 287s - loss: 0.0145 - val_loss: 0.0145 - 287s/epoch - 20s/step\n",
      "Epoch 162/350\n",
      "14/14 - 286s - loss: 0.0145 - val_loss: 0.0150 - 286s/epoch - 20s/step\n",
      "Epoch 163/350\n",
      "14/14 - 290s - loss: 0.0146 - val_loss: 0.0146 - 290s/epoch - 21s/step\n",
      "Epoch 164/350\n",
      "14/14 - 284s - loss: 0.0145 - val_loss: 0.0146 - 284s/epoch - 20s/step\n",
      "Epoch 165/350\n",
      "14/14 - 286s - loss: 0.0145 - val_loss: 0.0145 - 286s/epoch - 20s/step\n",
      "Epoch 166/350\n",
      "14/14 - 276s - loss: 0.0144 - val_loss: 0.0145 - 276s/epoch - 20s/step\n",
      "Epoch 167/350\n",
      "14/14 - 289s - loss: 0.0145 - val_loss: 0.0145 - 289s/epoch - 21s/step\n",
      "Epoch 168/350\n",
      "14/14 - 285s - loss: 0.0145 - val_loss: 0.0145 - 285s/epoch - 20s/step\n",
      "Epoch 169/350\n",
      "14/14 - 289s - loss: 0.0145 - val_loss: 0.0145 - 289s/epoch - 21s/step\n",
      "Epoch 170/350\n",
      "14/14 - 295s - loss: 0.0144 - val_loss: 0.0145 - 295s/epoch - 21s/step\n",
      "Epoch 171/350\n",
      "14/14 - 283s - loss: 0.0144 - val_loss: 0.0145 - 283s/epoch - 20s/step\n",
      "Epoch 172/350\n",
      "14/14 - 291s - loss: 0.0145 - val_loss: 0.0146 - 291s/epoch - 21s/step\n",
      "Epoch 173/350\n",
      "14/14 - 288s - loss: 0.0145 - val_loss: 0.0145 - 288s/epoch - 21s/step\n",
      "Epoch 174/350\n",
      "14/14 - 284s - loss: 0.0144 - val_loss: 0.0145 - 284s/epoch - 20s/step\n",
      "Epoch 175/350\n",
      "14/14 - 287s - loss: 0.0144 - val_loss: 0.0145 - 287s/epoch - 21s/step\n",
      "Epoch 176/350\n",
      "14/14 - 287s - loss: 0.0145 - val_loss: 0.0145 - 287s/epoch - 21s/step\n",
      "Epoch 177/350\n",
      "14/14 - 288s - loss: 0.0144 - val_loss: 0.0145 - 288s/epoch - 21s/step\n",
      "Epoch 178/350\n",
      "14/14 - 285s - loss: 0.0145 - val_loss: 0.0145 - 285s/epoch - 20s/step\n",
      "Epoch 179/350\n",
      "14/14 - 285s - loss: 0.0144 - val_loss: 0.0147 - 285s/epoch - 20s/step\n",
      "Epoch 180/350\n",
      "14/14 - 277s - loss: 0.0145 - val_loss: 0.0145 - 277s/epoch - 20s/step\n",
      "Epoch 181/350\n",
      "14/14 - 275s - loss: 0.0144 - val_loss: 0.0145 - 275s/epoch - 20s/step\n",
      "Epoch 182/350\n",
      "14/14 - 285s - loss: 0.0144 - val_loss: 0.0145 - 285s/epoch - 20s/step\n",
      "Epoch 183/350\n",
      "14/14 - 283s - loss: 0.0144 - val_loss: 0.0145 - 283s/epoch - 20s/step\n",
      "Epoch 184/350\n",
      "14/14 - 281s - loss: 0.0144 - val_loss: 0.0145 - 281s/epoch - 20s/step\n",
      "Epoch 185/350\n",
      "14/14 - 280s - loss: 0.0144 - val_loss: 0.0145 - 280s/epoch - 20s/step\n",
      "Epoch 186/350\n",
      "14/14 - 287s - loss: 0.0144 - val_loss: 0.0145 - 287s/epoch - 21s/step\n",
      "Epoch 187/350\n",
      "14/14 - 289s - loss: 0.0144 - val_loss: 0.0145 - 289s/epoch - 21s/step\n",
      "Epoch 188/350\n",
      "14/14 - 286s - loss: 0.0144 - val_loss: 0.0144 - 286s/epoch - 20s/step\n",
      "Epoch 189/350\n",
      "14/14 - 281s - loss: 0.0144 - val_loss: 0.0146 - 281s/epoch - 20s/step\n",
      "Epoch 190/350\n",
      "14/14 - 280s - loss: 0.0144 - val_loss: 0.0145 - 280s/epoch - 20s/step\n",
      "Epoch 191/350\n",
      "14/14 - 288s - loss: 0.0144 - val_loss: 0.0146 - 288s/epoch - 21s/step\n",
      "Epoch 192/350\n",
      "14/14 - 282s - loss: 0.0144 - val_loss: 0.0145 - 282s/epoch - 20s/step\n",
      "Epoch 193/350\n",
      "14/14 - 290s - loss: 0.0144 - val_loss: 0.0144 - 290s/epoch - 21s/step\n",
      "Epoch 194/350\n",
      "14/14 - 282s - loss: 0.0144 - val_loss: 0.0145 - 282s/epoch - 20s/step\n",
      "Epoch 195/350\n",
      "14/14 - 287s - loss: 0.0144 - val_loss: 0.0145 - 287s/epoch - 20s/step\n",
      "Epoch 196/350\n",
      "14/14 - 284s - loss: 0.0144 - val_loss: 0.0145 - 284s/epoch - 20s/step\n",
      "Epoch 197/350\n",
      "14/14 - 286s - loss: 0.0144 - val_loss: 0.0145 - 286s/epoch - 20s/step\n",
      "Epoch 198/350\n",
      "14/14 - 284s - loss: 0.0144 - val_loss: 0.0145 - 284s/epoch - 20s/step\n",
      "Epoch 199/350\n",
      "14/14 - 277s - loss: 0.0144 - val_loss: 0.0145 - 277s/epoch - 20s/step\n",
      "Epoch 200/350\n",
      "14/14 - 275s - loss: 0.0144 - val_loss: 0.0144 - 275s/epoch - 20s/step\n",
      "Epoch 201/350\n",
      "14/14 - 280s - loss: 0.0143 - val_loss: 0.0145 - 280s/epoch - 20s/step\n",
      "Epoch 202/350\n",
      "14/14 - 282s - loss: 0.0143 - val_loss: 0.0145 - 282s/epoch - 20s/step\n",
      "Epoch 203/350\n",
      "14/14 - 278s - loss: 0.0144 - val_loss: 0.0144 - 278s/epoch - 20s/step\n",
      "Epoch 204/350\n",
      "14/14 - 278s - loss: 0.0143 - val_loss: 0.0145 - 278s/epoch - 20s/step\n",
      "Epoch 205/350\n",
      "14/14 - 282s - loss: 0.0144 - val_loss: 0.0145 - 282s/epoch - 20s/step\n",
      "Epoch 206/350\n",
      "14/14 - 279s - loss: 0.0144 - val_loss: 0.0144 - 279s/epoch - 20s/step\n",
      "Epoch 207/350\n",
      "14/14 - 284s - loss: 0.0144 - val_loss: 0.0144 - 284s/epoch - 20s/step\n",
      "Epoch 208/350\n",
      "14/14 - 277s - loss: 0.0143 - val_loss: 0.0144 - 277s/epoch - 20s/step\n",
      "Epoch 209/350\n",
      "14/14 - 277s - loss: 0.0144 - val_loss: 0.0145 - 277s/epoch - 20s/step\n",
      "Epoch 210/350\n",
      "14/14 - 282s - loss: 0.0143 - val_loss: 0.0144 - 282s/epoch - 20s/step\n",
      "Epoch 211/350\n",
      "14/14 - 277s - loss: 0.0144 - val_loss: 0.0144 - 277s/epoch - 20s/step\n",
      "Epoch 212/350\n",
      "14/14 - 282s - loss: 0.0143 - val_loss: 0.0145 - 282s/epoch - 20s/step\n",
      "Epoch 213/350\n",
      "14/14 - 280s - loss: 0.0144 - val_loss: 0.0144 - 280s/epoch - 20s/step\n",
      "Epoch 214/350\n",
      "14/14 - 279s - loss: 0.0143 - val_loss: 0.0145 - 279s/epoch - 20s/step\n",
      "Epoch 215/350\n",
      "14/14 - 280s - loss: 0.0144 - val_loss: 0.0145 - 280s/epoch - 20s/step\n",
      "Epoch 216/350\n",
      "14/14 - 275s - loss: 0.0144 - val_loss: 0.0145 - 275s/epoch - 20s/step\n",
      "Epoch 217/350\n",
      "14/14 - 277s - loss: 0.0143 - val_loss: 0.0144 - 277s/epoch - 20s/step\n",
      "Epoch 218/350\n",
      "14/14 - 280s - loss: 0.0143 - val_loss: 0.0144 - 280s/epoch - 20s/step\n",
      "Epoch 219/350\n",
      "14/14 - 272s - loss: 0.0143 - val_loss: 0.0144 - 272s/epoch - 19s/step\n",
      "Epoch 220/350\n",
      "14/14 - 277s - loss: 0.0143 - val_loss: 0.0144 - 277s/epoch - 20s/step\n",
      "Epoch 221/350\n",
      "14/14 - 280s - loss: 0.0143 - val_loss: 0.0144 - 280s/epoch - 20s/step\n",
      "Epoch 222/350\n",
      "14/14 - 278s - loss: 0.0143 - val_loss: 0.0144 - 278s/epoch - 20s/step\n",
      "Epoch 223/350\n",
      "14/14 - 276s - loss: 0.0143 - val_loss: 0.0144 - 276s/epoch - 20s/step\n",
      "Epoch 224/350\n",
      "14/14 - 274s - loss: 0.0143 - val_loss: 0.0144 - 274s/epoch - 20s/step\n",
      "Epoch 225/350\n",
      "14/14 - 269s - loss: 0.0143 - val_loss: 0.0144 - 269s/epoch - 19s/step\n",
      "Epoch 226/350\n",
      "14/14 - 270s - loss: 0.0143 - val_loss: 0.0145 - 270s/epoch - 19s/step\n",
      "Epoch 227/350\n",
      "14/14 - 270s - loss: 0.0143 - val_loss: 0.0144 - 270s/epoch - 19s/step\n",
      "Epoch 228/350\n",
      "14/14 - 270s - loss: 0.0143 - val_loss: 0.0144 - 270s/epoch - 19s/step\n",
      "Epoch 229/350\n",
      "14/14 - 272s - loss: 0.0143 - val_loss: 0.0145 - 272s/epoch - 19s/step\n",
      "Epoch 230/350\n",
      "14/14 - 272s - loss: 0.0143 - val_loss: 0.0144 - 272s/epoch - 19s/step\n",
      "Epoch 231/350\n",
      "14/14 - 270s - loss: 0.0143 - val_loss: 0.0145 - 270s/epoch - 19s/step\n",
      "Epoch 232/350\n",
      "14/14 - 269s - loss: 0.0143 - val_loss: 0.0144 - 269s/epoch - 19s/step\n",
      "Epoch 233/350\n",
      "14/14 - 270s - loss: 0.0143 - val_loss: 0.0144 - 270s/epoch - 19s/step\n",
      "Epoch 234/350\n",
      "14/14 - 271s - loss: 0.0143 - val_loss: 0.0144 - 271s/epoch - 19s/step\n",
      "Epoch 235/350\n",
      "14/14 - 268s - loss: 0.0143 - val_loss: 0.0144 - 268s/epoch - 19s/step\n",
      "Epoch 236/350\n",
      "14/14 - 264s - loss: 0.0143 - val_loss: 0.0145 - 264s/epoch - 19s/step\n",
      "Epoch 237/350\n",
      "14/14 - 267s - loss: 0.0143 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 238/350\n",
      "14/14 - 264s - loss: 0.0143 - val_loss: 0.0144 - 264s/epoch - 19s/step\n",
      "Epoch 239/350\n",
      "14/14 - 265s - loss: 0.0143 - val_loss: 0.0144 - 265s/epoch - 19s/step\n",
      "Epoch 240/350\n",
      "14/14 - 265s - loss: 0.0143 - val_loss: 0.0145 - 265s/epoch - 19s/step\n",
      "Epoch 241/350\n",
      "14/14 - 268s - loss: 0.0143 - val_loss: 0.0144 - 268s/epoch - 19s/step\n",
      "Epoch 242/350\n",
      "14/14 - 265s - loss: 0.0143 - val_loss: 0.0144 - 265s/epoch - 19s/step\n",
      "Epoch 243/350\n",
      "14/14 - 262s - loss: 0.0143 - val_loss: 0.0144 - 262s/epoch - 19s/step\n",
      "Epoch 244/350\n",
      "14/14 - 266s - loss: 0.0143 - val_loss: 0.0144 - 266s/epoch - 19s/step\n",
      "Epoch 245/350\n",
      "14/14 - 263s - loss: 0.0143 - val_loss: 0.0144 - 263s/epoch - 19s/step\n",
      "Epoch 246/350\n",
      "14/14 - 265s - loss: 0.0143 - val_loss: 0.0144 - 265s/epoch - 19s/step\n",
      "Epoch 247/350\n",
      "14/14 - 265s - loss: 0.0143 - val_loss: 0.0144 - 265s/epoch - 19s/step\n",
      "Epoch 248/350\n",
      "14/14 - 261s - loss: 0.0143 - val_loss: 0.0144 - 261s/epoch - 19s/step\n",
      "Epoch 249/350\n",
      "14/14 - 262s - loss: 0.0143 - val_loss: 0.0144 - 262s/epoch - 19s/step\n",
      "Epoch 250/350\n",
      "14/14 - 260s - loss: 0.0143 - val_loss: 0.0144 - 260s/epoch - 19s/step\n",
      "Epoch 251/350\n",
      "14/14 - 261s - loss: 0.0143 - val_loss: 0.0144 - 261s/epoch - 19s/step\n",
      "Epoch 252/350\n",
      "14/14 - 262s - loss: 0.0143 - val_loss: 0.0144 - 262s/epoch - 19s/step\n",
      "Epoch 253/350\n",
      "14/14 - 260s - loss: 0.0143 - val_loss: 0.0144 - 260s/epoch - 19s/step\n",
      "Epoch 254/350\n",
      "14/14 - 263s - loss: 0.0143 - val_loss: 0.0144 - 263s/epoch - 19s/step\n",
      "Epoch 255/350\n",
      "14/14 - 259s - loss: 0.0143 - val_loss: 0.0144 - 259s/epoch - 19s/step\n",
      "Epoch 256/350\n",
      "14/14 - 271s - loss: 0.0143 - val_loss: 0.0144 - 271s/epoch - 19s/step\n",
      "Epoch 257/350\n",
      "14/14 - 266s - loss: 0.0143 - val_loss: 0.0144 - 266s/epoch - 19s/step\n",
      "Epoch 258/350\n",
      "14/14 - 263s - loss: 0.0143 - val_loss: 0.0144 - 263s/epoch - 19s/step\n",
      "Epoch 259/350\n",
      "14/14 - 267s - loss: 0.0143 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 260/350\n",
      "14/14 - 265s - loss: 0.0143 - val_loss: 0.0144 - 265s/epoch - 19s/step\n",
      "Epoch 261/350\n",
      "14/14 - 263s - loss: 0.0143 - val_loss: 0.0144 - 263s/epoch - 19s/step\n",
      "Epoch 262/350\n",
      "14/14 - 262s - loss: 0.0143 - val_loss: 0.0144 - 262s/epoch - 19s/step\n",
      "Epoch 263/350\n",
      "14/14 - 261s - loss: 0.0142 - val_loss: 0.0145 - 261s/epoch - 19s/step\n",
      "Epoch 264/350\n",
      "14/14 - 261s - loss: 0.0143 - val_loss: 0.0144 - 261s/epoch - 19s/step\n",
      "Epoch 265/350\n",
      "14/14 - 266s - loss: 0.0143 - val_loss: 0.0144 - 266s/epoch - 19s/step\n",
      "Epoch 266/350\n",
      "14/14 - 261s - loss: 0.0143 - val_loss: 0.0144 - 261s/epoch - 19s/step\n",
      "Epoch 267/350\n",
      "14/14 - 260s - loss: 0.0143 - val_loss: 0.0143 - 260s/epoch - 19s/step\n",
      "Epoch 268/350\n",
      "14/14 - 271s - loss: 0.0143 - val_loss: 0.0144 - 271s/epoch - 19s/step\n",
      "Epoch 269/350\n",
      "14/14 - 273s - loss: 0.0142 - val_loss: 0.0143 - 273s/epoch - 19s/step\n",
      "Epoch 270/350\n",
      "14/14 - 267s - loss: 0.0143 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 271/350\n",
      "14/14 - 270s - loss: 0.0142 - val_loss: 0.0144 - 270s/epoch - 19s/step\n",
      "Epoch 272/350\n",
      "14/14 - 264s - loss: 0.0143 - val_loss: 0.0144 - 264s/epoch - 19s/step\n",
      "Epoch 273/350\n",
      "14/14 - 267s - loss: 0.0143 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 274/350\n",
      "14/14 - 267s - loss: 0.0143 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 275/350\n",
      "14/14 - 260s - loss: 0.0142 - val_loss: 0.0144 - 260s/epoch - 19s/step\n",
      "Epoch 276/350\n",
      "14/14 - 262s - loss: 0.0142 - val_loss: 0.0144 - 262s/epoch - 19s/step\n",
      "Epoch 277/350\n",
      "14/14 - 261s - loss: 0.0142 - val_loss: 0.0144 - 261s/epoch - 19s/step\n",
      "Epoch 278/350\n",
      "14/14 - 261s - loss: 0.0143 - val_loss: 0.0144 - 261s/epoch - 19s/step\n",
      "Epoch 279/350\n",
      "14/14 - 261s - loss: 0.0142 - val_loss: 0.0144 - 261s/epoch - 19s/step\n",
      "Epoch 280/350\n",
      "14/14 - 264s - loss: 0.0142 - val_loss: 0.0144 - 264s/epoch - 19s/step\n",
      "Epoch 281/350\n",
      "14/14 - 262s - loss: 0.0142 - val_loss: 0.0144 - 262s/epoch - 19s/step\n",
      "Epoch 282/350\n",
      "14/14 - 262s - loss: 0.0142 - val_loss: 0.0144 - 262s/epoch - 19s/step\n",
      "Epoch 283/350\n",
      "14/14 - 281s - loss: 0.0142 - val_loss: 0.0143 - 281s/epoch - 20s/step\n",
      "Epoch 284/350\n",
      "14/14 - 287s - loss: 0.0142 - val_loss: 0.0145 - 287s/epoch - 21s/step\n",
      "Epoch 285/350\n",
      "14/14 - 286s - loss: 0.0143 - val_loss: 0.0144 - 286s/epoch - 20s/step\n",
      "Epoch 286/350\n",
      "14/14 - 283s - loss: 0.0142 - val_loss: 0.0144 - 283s/epoch - 20s/step\n",
      "Epoch 287/350\n",
      "14/14 - 286s - loss: 0.0142 - val_loss: 0.0144 - 286s/epoch - 20s/step\n",
      "Epoch 288/350\n",
      "14/14 - 280s - loss: 0.0142 - val_loss: 0.0143 - 280s/epoch - 20s/step\n",
      "Epoch 289/350\n",
      "14/14 - 280s - loss: 0.0142 - val_loss: 0.0144 - 280s/epoch - 20s/step\n",
      "Epoch 290/350\n",
      "14/14 - 287s - loss: 0.0142 - val_loss: 0.0144 - 287s/epoch - 20s/step\n",
      "Epoch 291/350\n",
      "14/14 - 281s - loss: 0.0143 - val_loss: 0.0144 - 281s/epoch - 20s/step\n",
      "Epoch 292/350\n",
      "14/14 - 280s - loss: 0.0142 - val_loss: 0.0144 - 280s/epoch - 20s/step\n",
      "Epoch 293/350\n",
      "14/14 - 275s - loss: 0.0143 - val_loss: 0.0144 - 275s/epoch - 20s/step\n",
      "Epoch 294/350\n",
      "14/14 - 280s - loss: 0.0143 - val_loss: 0.0144 - 280s/epoch - 20s/step\n",
      "Epoch 295/350\n",
      "14/14 - 277s - loss: 0.0143 - val_loss: 0.0144 - 277s/epoch - 20s/step\n",
      "Epoch 296/350\n",
      "14/14 - 271s - loss: 0.0143 - val_loss: 0.0143 - 271s/epoch - 19s/step\n",
      "Epoch 297/350\n",
      "14/14 - 275s - loss: 0.0142 - val_loss: 0.0144 - 275s/epoch - 20s/step\n",
      "Epoch 298/350\n",
      "14/14 - 275s - loss: 0.0143 - val_loss: 0.0144 - 275s/epoch - 20s/step\n",
      "Epoch 299/350\n",
      "14/14 - 276s - loss: 0.0142 - val_loss: 0.0144 - 276s/epoch - 20s/step\n",
      "Epoch 300/350\n",
      "14/14 - 277s - loss: 0.0142 - val_loss: 0.0144 - 277s/epoch - 20s/step\n",
      "Epoch 301/350\n",
      "14/14 - 279s - loss: 0.0142 - val_loss: 0.0143 - 279s/epoch - 20s/step\n",
      "Epoch 302/350\n",
      "14/14 - 277s - loss: 0.0142 - val_loss: 0.0144 - 277s/epoch - 20s/step\n",
      "Epoch 303/350\n",
      "14/14 - 276s - loss: 0.0142 - val_loss: 0.0144 - 276s/epoch - 20s/step\n",
      "Epoch 304/350\n",
      "14/14 - 270s - loss: 0.0143 - val_loss: 0.0144 - 270s/epoch - 19s/step\n",
      "Epoch 305/350\n",
      "14/14 - 270s - loss: 0.0142 - val_loss: 0.0143 - 270s/epoch - 19s/step\n",
      "Epoch 306/350\n",
      "14/14 - 268s - loss: 0.0142 - val_loss: 0.0144 - 268s/epoch - 19s/step\n",
      "Epoch 307/350\n",
      "14/14 - 269s - loss: 0.0142 - val_loss: 0.0144 - 269s/epoch - 19s/step\n",
      "Epoch 308/350\n",
      "14/14 - 271s - loss: 0.0142 - val_loss: 0.0144 - 271s/epoch - 19s/step\n",
      "Epoch 309/350\n",
      "14/14 - 268s - loss: 0.0142 - val_loss: 0.0144 - 268s/epoch - 19s/step\n",
      "Epoch 310/350\n",
      "14/14 - 267s - loss: 0.0142 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 311/350\n",
      "14/14 - 268s - loss: 0.0142 - val_loss: 0.0143 - 268s/epoch - 19s/step\n",
      "Epoch 312/350\n",
      "14/14 - 267s - loss: 0.0144 - val_loss: 0.0143 - 267s/epoch - 19s/step\n",
      "Epoch 313/350\n",
      "14/14 - 267s - loss: 0.0142 - val_loss: 0.0143 - 267s/epoch - 19s/step\n",
      "Epoch 314/350\n",
      "14/14 - 267s - loss: 0.0142 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 315/350\n",
      "14/14 - 267s - loss: 0.0142 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 316/350\n",
      "14/14 - 269s - loss: 0.0142 - val_loss: 0.0143 - 269s/epoch - 19s/step\n",
      "Epoch 317/350\n",
      "14/14 - 267s - loss: 0.0142 - val_loss: 0.0143 - 267s/epoch - 19s/step\n",
      "Epoch 318/350\n",
      "14/14 - 274s - loss: 0.0142 - val_loss: 0.0143 - 274s/epoch - 20s/step\n",
      "Epoch 319/350\n",
      "14/14 - 273s - loss: 0.0142 - val_loss: 0.0143 - 273s/epoch - 19s/step\n",
      "Epoch 320/350\n",
      "14/14 - 272s - loss: 0.0142 - val_loss: 0.0144 - 272s/epoch - 19s/step\n",
      "Epoch 321/350\n",
      "14/14 - 267s - loss: 0.0142 - val_loss: 0.0144 - 267s/epoch - 19s/step\n",
      "Epoch 322/350\n",
      "14/14 - 264s - loss: 0.0142 - val_loss: 0.0143 - 264s/epoch - 19s/step\n",
      "Epoch 323/350\n",
      "14/14 - 266s - loss: 0.0142 - val_loss: 0.0144 - 266s/epoch - 19s/step\n",
      "Epoch 324/350\n",
      "14/14 - 269s - loss: 0.0142 - val_loss: 0.0143 - 269s/epoch - 19s/step\n",
      "Epoch 325/350\n",
      "14/14 - 263s - loss: 0.0142 - val_loss: 0.0144 - 263s/epoch - 19s/step\n",
      "Epoch 326/350\n",
      "14/14 - 264s - loss: 0.0142 - val_loss: 0.0143 - 264s/epoch - 19s/step\n",
      "Epoch 327/350\n",
      "14/14 - 280s - loss: 0.0142 - val_loss: 0.0143 - 280s/epoch - 20s/step\n",
      "Epoch 328/350\n",
      "14/14 - 271s - loss: 0.0142 - val_loss: 0.0144 - 271s/epoch - 19s/step\n",
      "Epoch 329/350\n",
      "14/14 - 270s - loss: 0.0142 - val_loss: 0.0144 - 270s/epoch - 19s/step\n",
      "Epoch 330/350\n",
      "14/14 - 271s - loss: 0.0142 - val_loss: 0.0143 - 271s/epoch - 19s/step\n",
      "Epoch 331/350\n",
      "14/14 - 266s - loss: 0.0142 - val_loss: 0.0144 - 266s/epoch - 19s/step\n",
      "Epoch 332/350\n",
      "14/14 - 266s - loss: 0.0142 - val_loss: 0.0143 - 266s/epoch - 19s/step\n",
      "Epoch 333/350\n",
      "14/14 - 268s - loss: 0.0142 - val_loss: 0.0143 - 268s/epoch - 19s/step\n",
      "Epoch 334/350\n",
      "14/14 - 265s - loss: 0.0142 - val_loss: 0.0143 - 265s/epoch - 19s/step\n",
      "Epoch 335/350\n",
      "14/14 - 270s - loss: 0.0142 - val_loss: 0.0144 - 270s/epoch - 19s/step\n",
      "Epoch 336/350\n",
      "14/14 - 268s - loss: 0.0142 - val_loss: 0.0143 - 268s/epoch - 19s/step\n",
      "Epoch 337/350\n",
      "14/14 - 264s - loss: 0.0142 - val_loss: 0.0143 - 264s/epoch - 19s/step\n",
      "Epoch 338/350\n",
      "14/14 - 265s - loss: 0.0142 - val_loss: 0.0143 - 265s/epoch - 19s/step\n",
      "Epoch 339/350\n",
      "14/14 - 275s - loss: 0.0142 - val_loss: 0.0144 - 275s/epoch - 20s/step\n",
      "Epoch 340/350\n",
      "14/14 - 278s - loss: 0.0142 - val_loss: 0.0143 - 278s/epoch - 20s/step\n",
      "Epoch 341/350\n",
      "14/14 - 271s - loss: 0.0142 - val_loss: 0.0143 - 271s/epoch - 19s/step\n",
      "Epoch 342/350\n",
      "14/14 - 270s - loss: 0.0142 - val_loss: 0.0143 - 270s/epoch - 19s/step\n",
      "Epoch 343/350\n",
      "14/14 - 281s - loss: 0.0142 - val_loss: 0.0143 - 281s/epoch - 20s/step\n",
      "Epoch 344/350\n",
      "14/14 - 272s - loss: 0.0142 - val_loss: 0.0143 - 272s/epoch - 19s/step\n",
      "Epoch 345/350\n",
      "14/14 - 280s - loss: 0.0142 - val_loss: 0.0143 - 280s/epoch - 20s/step\n",
      "Epoch 346/350\n",
      "14/14 - 276s - loss: 0.0142 - val_loss: 0.0144 - 276s/epoch - 20s/step\n",
      "Epoch 347/350\n",
      "14/14 - 269s - loss: 0.0142 - val_loss: 0.0143 - 269s/epoch - 19s/step\n",
      "Epoch 348/350\n",
      "14/14 - 275s - loss: 0.0142 - val_loss: 0.0144 - 275s/epoch - 20s/step\n",
      "Epoch 349/350\n",
      "14/14 - 268s - loss: 0.0142 - val_loss: 0.0143 - 268s/epoch - 19s/step\n",
      "Epoch 350/350\n",
      "14/14 - 293s - loss: 0.0142 - val_loss: 0.0143 - 293s/epoch - 21s/step\n"
     ]
    }
   ],
   "source": [
    "ann_fit_hist = autoencoder.fit(df_data_only_inverted, df_data_only_inverted,\n",
    "                                       epochs=n_epoch,\n",
    "                                       callbacks=[checkpoint],\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=40000,\n",
    "                                       validation_split=0.10,\n",
    "                                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_combined = np.vstack((ann_fit_hist.history['loss'], ann_fit_hist.history['val_loss'])).T\n",
    "np.savetxt('ann_network_loss.txt', loss_combined)\n",
    "i_best = np.argmin(ann_fit_hist.history['val_loss'])\n",
    "plt.plot(ann_fit_hist.history['loss'], label='Train')\n",
    "plt.plot(ann_fit_hist.history['val_loss'], label='Validation')\n",
    "plt.axvline(np.arange(n_epoch)[i_best], ls='--', color='black', alpha=0.5)\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "plt.ylim(np.nanmin(loss_combined)*0.95, np.nanpercentile(loss_combined, 99))\n",
    "plt.xlim(-1, n_epoch)\n",
    "plt.grid(ls='--', alpha=0.2, color='black')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('ann_network_loss.png', dpi=250)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model_file = 'model_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 10 with the loss (0.0179).\n",
      "Saving selected model weights\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 25 with the loss (0.0177).\n",
      "Saving selected model weights\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 50 with the loss (0.0160).\n",
      "Saving selected model weights\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 100 with the loss (0.0149).\n",
      "Saving selected model weights\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 150 with the loss (0.0150).\n",
      "Saving selected model weights\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 200 with the loss (0.0144).\n",
      "Saving selected model weights\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 250 with the loss (0.0144).\n",
      "Saving selected model weights\n",
      "----------------------------------------------------------------\n",
      "Restoring epoch 300 with the loss (0.0144).\n",
      "Saving selected model weights\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "        # recover weights of the selected model and compute predictions\n",
    "for i_best in [10, 25, 50, 100, 150, 200, 250, 300]:\n",
    "\n",
    "    h5_weight_files = glob('/data/praveen/autoencoder-run-2/'+'ann_model_run_{:03.0f}-*-*.h5'.format(i_best))\n",
    "\n",
    "    if len(h5_weight_files) == 1:\n",
    "        print('----------------------------------------------------------------')\n",
    "        print('Restoring epoch {:.0f} with the loss ({:.4f}).'.format(i_best, ann_fit_hist.history['val_loss'][i_best-1]))\n",
    "        autoencoder.load_weights(h5_weight_files[0], by_name=True)\n",
    "\n",
    "        sub_dir = 'epoch_{:03.0f}'.format(i_best)\n",
    "        os.system('mkdir ' + sub_dir)\n",
    "        os.chdir(sub_dir)\n",
    "\n",
    "        print('Saving selected model weights')\n",
    "        autoencoder.save_weights(out_model_file)\n",
    "\n",
    "        #print('Predicting values')\n",
    "        #processed_data_all = autoencoder.predict(df_data_only_inverted, verbose=2, batch_size=20)\n",
    "\n",
    "        #plot_sample_spectra()\n",
    "\n",
    "        #print('')\n",
    "        #os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dr3 = pd.read_csv(\"/data/praveen/results/dr3_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sobject_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>4449</th>\n",
       "      <th>4450</th>\n",
       "      <th>4451</th>\n",
       "      <th>4452</th>\n",
       "      <th>4453</th>\n",
       "      <th>4454</th>\n",
       "      <th>4455</th>\n",
       "      <th>4456</th>\n",
       "      <th>4457</th>\n",
       "      <th>4458</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131116000501002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131116000501004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131116000501005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131116000501006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131116000501007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sobject_id    0    1    2    3    4    5    6    7    8  ...  4449  \\\n",
       "0  131116000501002  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "1  131116000501004  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "2  131116000501005  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "3  131116000501006  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "4  131116000501007  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...   1.0   \n",
       "\n",
       "   4450  4451  4452  4453  4454  4455  4456  4457  4458  \n",
       "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "1   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "2   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "3   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "4   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "\n",
       "[5 rows x 4460 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dr3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10908/1450910931.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_dr3_data_only = df_dr3.drop([\"sobject_id\"],1)\n"
     ]
    }
   ],
   "source": [
    "df_dr3_data_only = df_dr3.drop([\"sobject_id\"],1)\n",
    "df_dr3_data_only.head()\n",
    "df_dr3_data_only_inverted = 1 - df_dr3_data_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4449</th>\n",
       "      <th>4450</th>\n",
       "      <th>4451</th>\n",
       "      <th>4452</th>\n",
       "      <th>4453</th>\n",
       "      <th>4454</th>\n",
       "      <th>4455</th>\n",
       "      <th>4456</th>\n",
       "      <th>4457</th>\n",
       "      <th>4458</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  4449  4450  4451  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "   4452  4453  4454  4455  4456  4457  4458  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 4459 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dr3_data_only_inverted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting values\n",
      "15/15 - 105s - 105s/epoch - 7s/step\n"
     ]
    }
   ],
   "source": [
    "print('Predicting values')\n",
    "processed_data_all = autoencoder.predict(df_dr3_data_only_inverted, verbose=2, batch_size=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0007062 , -0.00076971, -0.00035594, ..., -0.00080945,\n",
       "         0.00077041, -0.00093736],\n",
       "       [ 0.00071003, -0.00021888, -0.00074688, ..., -0.00040598,\n",
       "         0.00012151,  0.00020568],\n",
       "       [ 0.00097113, -0.0002512 , -0.00065225, ..., -0.00077877,\n",
       "         0.00042948, -0.00024286],\n",
       "       ...,\n",
       "       [ 0.00079654, -0.00054027, -0.00039906, ..., -0.00080656,\n",
       "         0.00067842, -0.00044822],\n",
       "       [ 0.000704  , -0.00025581, -0.00071063, ..., -0.00050182,\n",
       "         0.0006199 , -0.00086663],\n",
       "       [ 0.00090816, -0.00053902, -0.00033442, ..., -0.00063907,\n",
       "         0.00063569, -0.00062414]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_all = pd.DataFrame(processed_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4449</th>\n",
       "      <th>4450</th>\n",
       "      <th>4451</th>\n",
       "      <th>4452</th>\n",
       "      <th>4453</th>\n",
       "      <th>4454</th>\n",
       "      <th>4455</th>\n",
       "      <th>4456</th>\n",
       "      <th>4457</th>\n",
       "      <th>4458</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000706</td>\n",
       "      <td>-0.000770</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>-0.000937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000710</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000971</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>-0.000482</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>-0.000617</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001088</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>-0.000589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>-0.000424</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>-0.000289</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000690</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>-0.000235</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>-0.000651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.000706 -0.000770 -0.000356  0.000360 -0.000050 -0.000876  0.000122   \n",
       "1  0.000710 -0.000219 -0.000747  0.000530  0.000823 -0.000009  0.000586   \n",
       "2  0.000971 -0.000251 -0.000652  0.000780  0.000541 -0.000482  0.000262   \n",
       "3  0.001088 -0.000359 -0.000646  0.000193  0.000507 -0.000113  0.000173   \n",
       "4  0.000690 -0.000478 -0.000466  0.000209  0.000357 -0.000235  0.000467   \n",
       "\n",
       "       7         8         9     ...      4449      4450      4451      4452  \\\n",
       "0  0.000082 -0.000170 -0.000392  ...  0.000756  0.000128  0.000213  0.000091   \n",
       "1  0.000549 -0.000094 -0.000656  ...  0.000082 -0.000362 -0.000344 -0.000076   \n",
       "2  0.000376  0.000391 -0.000594  ...  0.000400 -0.000088  0.000155  0.000066   \n",
       "3  0.000856  0.000919 -0.000589  ...  0.000086 -0.000373 -0.000424  0.000150   \n",
       "4  0.000570  0.000634 -0.000537  ...  0.000138 -0.000177 -0.000386  0.000305   \n",
       "\n",
       "       4453      4454      4455      4456      4457      4458  \n",
       "0  0.000872  0.000331  0.000736 -0.000809  0.000770 -0.000937  \n",
       "1  0.000664 -0.000703  0.000716 -0.000406  0.000122  0.000206  \n",
       "2  0.000700 -0.000617  0.000331 -0.000779  0.000429 -0.000243  \n",
       "3  0.000460 -0.000289  0.000019 -0.000651  0.000062 -0.000111  \n",
       "4  0.000235 -0.000514  0.000277 -0.000678  0.000737 -0.000651  \n",
       "\n",
       "[5 rows x 4459 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_all.to_csv(\"/data/praveen/results/dr3_full_predicted_inverted.csv\",index=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3aa1867c408c14afe351cfd5d1be181646832c7964023d6477fd09a3c85411b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
